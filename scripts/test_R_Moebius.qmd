 ---
title: Testing MoEBius
output: html_document
---



## Simulation of an easy data set for Moebius


```{r}
# Load dependencies and source code
source("R/F_simulations.R")
source("R/MoEBIUS.R")

# ------------------------------
# 1. Simulate synthetic data
# ------------------------------
set.seed(123)

# Define parameters
N <- 500     # number of observations
p <- 10      # number of features
K <- 3       # number of row clusters
Q <- 2       # number of column clusters

# Simulate data
data_sim <- simu_CCLBM_supervised(N = N, p = p, K = K, Q = Q)

```

### Representation of the simulated data (3 row clusters and two column clusters)

```{r}
library(ggplot2)
library(plotly)
library(RColorBrewer)

# Extract simulation components
X <- data_sim$X
Z <- data_sim$Z_classif
W <- data_sim$W_classif
y <- as.vector(data_sim$y)

# -----------------------------
# 1. Heatmap of X with ordering
# -----------------------------

# Reorder rows and columns
row_order <- order(Z)
col_order <- order(W[Z[row_order[1]], ])  # W depends on Z[k,]

X_reordered <- X[row_order, col_order]

# Plot heatmap
heatmap(X_reordered,
        Rowv = NA, Colv = NA,
        scale = "none", col = colorRampPalette(brewer.pal(9, "YlGnBu"))(100),
        main = "Reordered Heatmap of X (by Z and W)")

# -----------------------------
# 2. PCA with y as third dimension
# -----------------------------

# Run PCA
pca <- prcomp(X, scale. = TRUE)
pca_df <- data.frame(PC1 = pca$x[,1],
                     PC2 = pca$x[,2],
                     y = y,
                     cluster = factor(Z))

# 3D Scatter plot with y as color
plot_ly(data = pca_df,
        x = ~PC1, y = ~PC2, z = ~y,
        color = ~cluster,
        colors = RColorBrewer::brewer.pal(length(unique(Z)), "Set2"),
        type = 'scatter3d',
        mode = 'markers') %>%
  layout(title = "3D Visualization: PCA(X) + y",
         scene = list(
           xaxis = list(title = 'PC1'),
           yaxis = list(title = 'PC2'),
           zaxis = list(title = 'y')))
```

## Learning set error


```{r}

# Extract predictors and response
X <- data_sim$X
y <- as.vector(data_sim$y)

# ------------------------------
# 2. Fit MoEBIUS model
# ------------------------------
model_fit <- MoEBIUS_reg(
  X = X,
  y = y,
  K_set = K,           # fixed K
  Q_set = Q,           # fixed Q
  iter_max = 2000,       # number of EM iterations
  learning_rate = 7e-2,# optional: slightly higher LR
  Cross_val = FALSE    # disable cross-validation
)

# ------------------------------
# 3. Predict on training data
# ------------------------------
y_pred <- prediction_MoEBIUS_reg(model_fit$best_ELBO, X)

# ------------------------------
# 4. Evaluate model
# ------------------------------
mse <- mean((y - y_pred)^2)
cat("MSE on training data:", mse, "\n")

# Optional: plot predicted vs true
plot(y, y_pred,
     main = "MoEBIUS: Predicted vs True y",
     xlab = "True y", ylab = "Predicted y",
     pch = 19, col = "steelblue")
abline(a = 0, b = 1, col = "red", lwd = 2)

```


```{r}
# --- 1. Simulate Training Data ---
set.seed(123)
K <- 3; Q <- 2; p <- 10; N_train <- 500; N_test <- 700

data <- simu_CCLBM_supervised(N = N_train+N_test, p = p, K = K, Q = Q)
train_index<-sample(1:(N_train+N_test),N_train,replace=FALSE)


X_train <- data$X[train_index,]
y_train <- as.vector(data$y[train_index])

X_test <- data$X[-train_index,]
y_test <- as.vector(data$y[-train_index])

# --- 3. Fit MoEBIUS on Training Data ---
model <- MoEBIUS_reg(
  X = X_train,
  y = y_train,
  K_set = K,
  Q_set = Q,
  learning_rate = 7e-2,
  iter_max = 1000,
  Cross_val = FALSE
)

# --- 4a. Predict on Test Set ---
y_pred_test <- prediction_MoEBIUS_reg(model$best_ELBO, X_test)
# --- 4b. Predict on Training Set ---
y_pred_train <- prediction_MoEBIUS_reg(model$best_ELBO, X_train)

# --- 5b. Compute Train MSE ---
mse_train <- mean((y_train - y_pred_train)^2)
# --- 5c. Compute Train MSE ---
mse_test <- mean((y_test - y_pred_test)^2)

# --- 6. Display Summary Table ---
summary_df <- data.frame(
  N_train = N_train,
  N_test = N_test,
  p = p,
  K = K,
  Q = Q,
  iter_max = model$best_ELBO$iter_max,
  learning_rate = model$best_ELBO$learning_rate,
  MSE_train = round(mse_train, 4),
  MSE_test = round(mse_test, 4)
)

print(summary_df)
```
## Exporting simulated data for comparison with others MoEs



```{r}
write.csv(X_train, "X_train.csv", row.names = FALSE)
write.csv(X_test, "X_test.csv", row.names = FALSE)
write.csv(data.frame(y = y_train), "y_train.csv", row.names = FALSE)
write.csv(data.frame(y = y_test),  "y_test.csv",  row.names = FALSE)
```


## Runing python MoEs



### Running a simple mixture of experts

```{python}
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.decomposition import PCA

# Define a dense linear layer without sparsity
class DenseLinear(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.linear = nn.Linear(input_dim, output_dim)  # Standard linear layer
    
    def forward(self, x):
        return self.linear(x)  # Apply linear transformation

# Define the Mixture of Regressions (MoR) model without sparsity
class MixtureOfRegressions(nn.Module):
    def __init__(self, input_dim, output_dim, num_experts=3):
        super().__init__()
        self.num_experts = num_experts  # Number of experts
        self.gate_weights = nn.Parameter(torch.ones(num_experts) / num_experts)  # Fixed proportions
        
        # Create a list of dense linear experts
        self.experts = nn.ModuleList([DenseLinear(input_dim, output_dim) for _ in range(num_experts)])
    
    def forward(self, x):
        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=1)  # Compute outputs from all experts
        output = torch.sum(self.gate_weights.unsqueeze(0).unsqueeze(-1) * expert_outputs, dim=1)  # Weighted sum of expert outputs
        return output
```


#### Loading the data 


```{python}
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error

# --- 1. Load R-exported data ---
X_train = pd.read_csv("X_train.csv").values.astype(np.float32)
X_test  = pd.read_csv("X_test.csv").values.astype(np.float32)
y_train = pd.read_csv("y_train.csv")["y"].values.astype(np.float32)
y_test  = pd.read_csv("y_test.csv")["y"].values.astype(np.float32)

# Convert to torch tensors
X_train_torch = torch.tensor(X_train)
y_train_torch = torch.tensor(y_train).unsqueeze(1)
X_test_torch  = torch.tensor(X_test)
y_test_torch  = torch.tensor(y_test).unsqueeze(1)
```

```{python}
#| echo: true
#| output: false
#| warning: false
#| message: false

# Define model
input_dim = X_train.shape[1]
output_dim = 1
num_experts = 3

model = MixtureOfRegressions(input_dim, output_dim, num_experts)

# Loss and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# Training loop
n_epochs = 200
for epoch in range(n_epochs):
    model.train()
    optimizer.zero_grad()
    y_pred = model(X_train_torch)
    loss = criterion(y_pred, y_train_torch)
    loss.backward()
    optimizer.step()
    
    if epoch % 20 == 0 or epoch == n_epochs - 1:
        print(f"Epoch {epoch} | Train Loss: {loss.item():.4f}")
```
```{python}
#| echo: true
#| output: false
#| warning: false
#| message: false
model.eval() #
##
with torch.no_grad():
    y_train_pred = model(X_train_torch).squeeze().numpy()
    y_test_pred = model(X_test_torch).squeeze().numpy()

mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)

print("\nâœ… Mixture of Regressions:")
print(f"Train MSE: {mse_train:.4f}")
print(f"Test  MSE: {mse_test:.4f}")
```

### Sparse MoE


```{python}

class SparseMixtureOfRegressions(nn.Module):
    def __init__(self, input_dim, output_dim, num_experts=3):
        super().__init__()
        self.num_experts = num_experts
        self.gate_weights = nn.Parameter(torch.ones(num_experts) / num_experts)  # fixed/uniform gate
        self.experts = nn.ModuleList([nn.Linear(input_dim, output_dim) for _ in range(num_experts)])
    
    def forward(self, x):
        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=1)
        output = torch.sum(self.gate_weights.unsqueeze(0).unsqueeze(-1) * expert_outputs, dim=1)
        return output

    def l1_penalty(self):
        return sum(torch.sum(torch.abs(expert.weight)) for expert in self.experts)
```




```{python}
def train_sparse_moe(model, X_train, y_train, X_test, y_test, 
                     n_epochs=200, lr=1e-2, l1_lambda=1e-2):
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()
    
    for epoch in range(n_epochs):
        model.train()
        optimizer.zero_grad()
        y_pred = model(X_train)
        loss = criterion(y_pred, y_train) + l1_lambda * model.l1_penalty()
        loss.backward()
        optimizer.step()
        
        if epoch % 20 == 0 or epoch == n_epochs - 1:
            print(f"Epoch {epoch} | Loss: {loss.item():.4f}")
    
    # Evaluation
    model.eval()
    with torch.no_grad():
        y_train_pred = model(X_train).squeeze().numpy()
        y_test_pred = model(X_test).squeeze().numpy()
    
    mse_train = mean_squared_error(y_train.squeeze().numpy(), y_train_pred)
    mse_test = mean_squared_error(y_test.squeeze().numpy(), y_test_pred)
    
    return mse_train, mse_test, y_train_pred, y_test_pred
```

```{python}
# Assume you've already loaded the CSV data into:
# X_train_torch, y_train_torch, X_test_torch, y_test_torch

input_dim = X_train.shape[1]
output_dim = 1
num_experts = 3

sparse_model = SparseMixtureOfRegressions(input_dim, output_dim, num_experts)

mse_train, mse_test, y_hat_train, y_hat_test = train_sparse_moe(
    model=sparse_model,
    X_train=X_train_torch,
    y_train=y_train_torch,
    X_test=X_test_torch,
    y_test=y_test_torch,
    n_epochs=1000,
    lr=0.01,
    l1_lambda=3e-2  # adjust this to control sparsity
)

print("\nâœ… Sparse MoE Results:")
print(f"Train MSE: {mse_train:.4f}")
print(f"Test  MSE: {mse_test:.4f}")
```


### Winner Take All Sparse MoE

```{python}
class WTASparseMoE(nn.Module):
    def __init__(self, input_dim, output_dim, num_experts=3):
        super().__init__()
        self.num_experts = num_experts
        self.gate = nn.Linear(input_dim, num_experts)  # can be simple linear
        self.experts = nn.ModuleList([nn.Linear(input_dim, output_dim) for _ in range(num_experts)])
    
    def forward(self, x):
        gate_scores = self.gate(x)  # (batch, num_experts)
        winner_idx = gate_scores.argmax(dim=1)  # (batch,)
        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=1)  # (batch, num_experts, 1)
        output = expert_outputs[torch.arange(x.size(0)), winner_idx, :]  # select only the winning expert
        return output
    
    def l1_penalty(self):
        return sum(torch.sum(torch.abs(expert.weight)) for expert in self.experts)
```




```{python}
def train_wta_sparse_moe(model, X_train, y_train, X_test, y_test, 
                         n_epochs=300, lr=1e-2, l1_lambda=1e-2):
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()

    for epoch in range(n_epochs):
        model.train()
        optimizer.zero_grad()
        y_pred = model(X_train)
        loss = criterion(y_pred, y_train) + l1_lambda * model.l1_penalty()
        loss.backward()
        optimizer.step()

        if epoch % 20 == 0 or epoch == n_epochs - 1:
            print(f"Epoch {epoch} | Loss: {loss.item():.4f}")
    
    # Evaluation
    model.eval()
    with torch.no_grad():
        y_train_pred = model(X_train).squeeze().numpy()
        y_test_pred = model(X_test).squeeze().numpy()

    mse_train = mean_squared_error(y_train.squeeze().numpy(), y_train_pred)
    mse_test = mean_squared_error(y_test.squeeze().numpy(), y_test_pred)

    return mse_train, mse_test, y_train_pred, y_test_pred
```

```{python}
wta_sparse_model = WTASparseMoE(input_dim, output_dim, num_experts=3)

mse_train_ws, mse_test_ws, y_hat_train_ws, y_hat_test_ws = train_wta_sparse_moe(
    model=wta_sparse_model,
    X_train=X_train_torch,
    y_train=y_train_torch,
    X_test=X_test_torch,
    y_test=y_test_torch,
    n_epochs=1000,
    lr=0.01,
    l1_lambda=1e-2  # tune this!
)

print("\nðŸ¥‡ðŸ”ª WTA Sparse MoE:")
print(f"Train MSE: {mse_train_ws:.4f}")
print(f"Test  MSE: {mse_test_ws:.4f}")
```
```{python}
for i, expert in enumerate(wta_sparse_model.experts):
    plt.plot(expert.weight.detach().cpu().numpy().flatten(), label=f"Expert {i+1}")
plt.axhline(0, color='gray', linestyle='--')
plt.legend()
plt.title("WTA Sparse MoE: Expert Weights")
plt.show()
```


## MoEBIUS in pytorch

```{python}
def update_pi_CCLBM(X, tau, pi_k, learning_rate):
    """
    Update the pi_k parameters (softmax cluster projection) in MoEBIUS.

    Args:
        X (torch.Tensor): (N, p) data matrix
        tau (torch.Tensor): (N, K) current soft assignments
        pi_k (torch.Tensor): (p, K) projection weights
        learning_rate (float): step size

    Returns:
        Updated pi_k (torch.Tensor)
    """
    # 1. Compute logits and soft cluster probs
    logits = X @ pi_k  # (N, K)
    soft_tau = torch.softmax(logits, dim=1)  # softmax over rows

    # 2. Compute gradient
    grad = X.T @ (tau - soft_tau)  # (p, K)

    # 3. Update
    N = X.shape[0]
    pi_k = pi_k + (learning_rate / N) * grad

    return pi_k
```

```{python}
import torch

# Assume: X_train_torch is already loaded, shape (N, p)
N, p = X_train_torch.shape
K = 3  # for example, number of clusters

# Initialize tau (soft cluster assignments): shape (N, K)
tau = torch.softmax(torch.randn(N, K), dim=1)  # randomly initialized soft assignments

# Initialize pi_k (projection weights): shape (p, K)
pi_k = torch.randn(p, K, requires_grad=False)
```

```{python}
# Perform one update step
learning_rate = 0.07
pi_k_new = update_pi_CCLBM(X_train_torch, tau, pi_k, learning_rate)

# Check results
print("Old Ï€_k mean:", pi_k.mean().item())
print("New Ï€_k mean:", pi_k_new.mean().item())
print("Ï€_k change (L2 norm):", torch.norm(pi_k_new - pi_k).item())
```

#### update beta

```{python}
def update_beta_CCLBM(X, y, tau, nu, beta_init=None, reg=1e-6):
    """
    Updates beta for each cluster (expert) in MoEBIUS using weighted ridge regression.

    Args:
        X (Tensor): (N, p) data matrix
        y (Tensor): (N, 1) response
        tau (Tensor): (N, K) soft row assignments
        nu (Tensor): (K, p, Q) column cluster assignments
        beta_init (Tensor or None): optional initial beta (K, Q)
        reg (float): regularization strength (small ridge term)

    Returns:
        beta (Tensor): (K, Q) regression weights for each cluster
    """
    N, p = X.shape
    K, _, Q = nu.shape
    beta = torch.zeros(K, Q)

    for k in range(K):
        # Create weighted design matrix for cluster k
        D_tau = torch.diag(tau[:, k])
        X_tilde = nu[k].T @ X.T @ D_tau  # shape: (Q, N)

        # Regularized least squares
        XtX = X_tilde @ X @ nu[k] + reg * torch.eye(Q)
        Xty = X_tilde @ y

        beta_k = torch.linalg.solve(XtX, Xty).squeeze()  # (Q,)
        beta[k] = beta_k

    return beta
```




```{python}
# Fake Î½ for testing: assign features randomly to Q groups
Q = 2
K = tau.shape[1]
nu = torch.zeros((K, p, Q))

for k in range(K):
    # Random one-hot assignment for each feature j âˆˆ {1,...,p}
    group_assignments = torch.randint(0, Q, (p,))
    nu[k, torch.arange(p), group_assignments] = 1

# Update Î²
beta = update_beta_CCLBM(X_train_torch, y_train_torch, tau, nu)

print("Updated Î² shape:", beta.shape)
print(beta)
```

#### update tau

```{python}
def update_tau_CCLBM(X, y, pi_k, beta, nu, sigma2_k):
    """
    Update Ï„ (soft row cluster assignments) in MoEBIUS.

    Args:
        X (Tensor): (N, p)
        y (Tensor): (N, 1)
        pi_k (Tensor): (p, K)
        beta (Tensor): (K, Q)
        nu (Tensor): (K, p, Q)
        sigma2_k (Tensor): (K,)
    
    Returns:
        tau (Tensor): (N, K)
    """
    N, p = X.shape
    K, Q = beta.shape
    logits = X @ pi_k  # shape: (N, K)
    prior_probs = torch.softmax(logits, dim=1)  # shape: (N, K)

    log_y_prob = torch.zeros(N, K)

    for k in range(K):
        # X @ nu[k] â†’ shape: (N, Q)
        X_proj = X @ nu[k]  # (N, Q)
        y_pred = X_proj @ beta[k]  # (N,)
        var_k = sigma2_k[k]
        # Log density under normal: log N(y | XÎ², ÏƒÂ²)
        log_pdf = -0.5 * torch.log(2 * torch.pi * var_k) - 0.5 * ((y.squeeze() - y_pred) ** 2) / var_k
        log_y_prob[:, k] = log_pdf

    # Combine prior and likelihood (in log-space)
    log_prior = torch.log(prior_probs + 1e-10)
    Tik = log_y_prob + log_prior

    # Apply row-wise softmax to normalize (back to Ï„)
    tau = torch.softmax(Tik, dim=1)  # (N, K)
    return tau
```


##### Example of usage

```{python}
# Assume: pi_k, beta, nu, sigma2_k are already estimated (from earlier steps)
# If not, generate dummy values:

sigma2_k = torch.ones(K)  # assume unit variance
tau_new = update_tau_CCLBM(X_train_torch, y_train_torch, pi_k, beta, nu, sigma2_k)

print("Ï„ shape:", tau_new.shape)
print("Row-wise sums (should â‰ˆ 1):", tau_new.sum(dim=1)[:5])
```


#### Update of Sigma

```{python}
def update_sigma_CCLBM(X, y, tau, beta, nu):
    """
    Update sigmaÂ²_k (variance per expert) in MoEBIUS.

    Args:
        X (Tensor): (N, p)
        y (Tensor): (N, 1)
        tau (Tensor): (N, K)
        beta (Tensor): (K, Q)
        nu (Tensor): (K, p, Q)

    Returns:
        sigma2_k (Tensor): (K,)
    """
    N, p = X.shape
    K, Q = beta.shape
    sigma2_k = torch.zeros(K)

    for k in range(K):
        X_proj = X @ nu[k]  # (N, Q)
        y_pred = X_proj @ beta[k]  # (N,)
        residuals = (y.squeeze() - y_pred) ** 2  # (N,)
        weighted_residuals = tau[:, k] * residuals
        sigma2_k[k] = weighted_residuals.sum() / (tau[:, k].sum() + 1e-10)

    return sigma2_k
```


##### Example of usage

```{python}
sigma2_k = update_sigma_CCLBM(X_train_torch, y_train_torch, tau_new, beta, nu)

print("Updated ÏƒÂ² per expert:", sigma2_k)
```




#### Update nu (most complex part...)

```{python}
def update_nu_CCLBM2(X, y, tau, beta, rho_ks, sigma2_k):
    """
    Update Î½ (column cluster assignments) for MoEBIUS.

    Args:
        X (Tensor): (N, p)
        y (Tensor): (N, 1)
        tau (Tensor): (N, K)
        beta (Tensor): (K, Q)
        rho_ks (Tensor): (K, Q)
        sigma2_k (Tensor): (K,)

    Returns:
        nu (Tensor): (K, p, Q) â€” updated feature group assignments
    """
    N, p = X.shape
    K, Q = beta.shape
    log_2pi = np.log(2 * np.pi)

    V_kjs = torch.zeros(K, p, Q)

    for k in range(K):
        for j in range(p):
            for s in range(Q):
                # Build nu_tilde: assign feature j to group s
                nu_tilde = torch.zeros(p, Q)
                nu_tilde[:, :] = 0
                nu_tilde[j, s] = 1.0  # one-hot for feature j
                
                # Compute predicted y
                X_proj = X @ nu_tilde  # (N, Q), only 1 col active
                y_pred = X_proj @ beta[k]  # (N,)

                # Log-likelihood: log N(y | y_pred, ÏƒÂ²_k)
                var_k = sigma2_k[k]
                log_pdf = -0.5 * torch.log(var_k) - 0.5 * ((y.squeeze() - y_pred) ** 2) / var_k
                weighted_log_pdf = tau[:, k] * log_pdf

                # Add log prior
                log_prior = torch.log(rho_ks[k, s] + 1e-10)
                V_kjs[k, j, s] = log_prior + weighted_log_pdf.sum()

    # Softmax over s for each (k, j)
    V_kjs_softmax = torch.softmax(V_kjs, dim=2)  # dim=2: Q

    return V_kjs_softmax  # Î½[k, j, s]
```


##### Example of usage 

```{python}
# Simulate prior rho_ks if not available
rho_ks = torch.ones(K, Q) / Q

nu = update_nu_CCLBM2(X_train_torch, y_train_torch, tau_new, beta, rho_ks, sigma2_k)

print("Î½ shape:", nu.shape)  # Should be (K, p, Q)
print("Sum over Q (should be â‰ˆ1):", nu.sum(dim=2)[:5])
```
#### Rho update

```{python}
def update_rho_CCLBM(nu):
    """
    Update rho_ks (column group priors) for MoEBIUS.
    Args:
        nu (Tensor): (K, p, Q) current soft assignments of features to column groups
    Returns:
        rho_ks (Tensor): (K, Q) â€” updated priors per cluster over column groups
    """
    K, p, Q = nu.shape
    # Sum over features j for each group s, then normalize by p
    rho_ks = nu.sum(dim=1) / p  # (K, Q)
    # Numerical stability
    rho_ks = torch.clamp(rho_ks, min=1e-10)
    return rho_ks
```


##### Usage example 

```{python}
rho_ks = update_rho_CCLBM(nu)

print("Ï_ks shape:", rho_ks.shape)
print("Sum over groups per expert (should be â‰¤ 1 if clamped):", rho_ks.sum(dim=1))
```


#### ELBO 

```{python}
def ELBO_CCLBM(X, y, tau, pi_k, beta, nu, rho_ks, sigma2_k):
    """
    Compute ELBO (Evidence Lower Bound) for current MoEBIUS state.

    Args:
        X (Tensor): (N, p)
        y (Tensor): (N, 1)
        tau (Tensor): (N, K)
        pi_k (Tensor): (p, K)
        beta (Tensor): (K, Q)
        nu (Tensor): (K, p, Q)
        rho_ks (Tensor): (K, Q)
        sigma2_k (Tensor): (K,)

    Returns:
        elbo (float): scalar ELBO value
    """
    N, p = X.shape
    K, Q = beta.shape
    elbo = 0.0

    # --- 1. Likelihood term: log p(y | X, Î½_k, Î²_k, ÏƒÂ²_k)
    for k in range(K):
        X_proj = X @ nu[k]  # (N, Q)
        y_pred = X_proj @ beta[k]  # (N,)
        var_k = sigma2_k[k]
        log_lik = -0.5 * torch.log(2 * torch.pi * var_k) - 0.5 * ((y.squeeze() - y_pred) ** 2) / var_k
        elbo += (tau[:, k] * log_lik).sum()

    # --- 2. Prior over Z: Ï„ * log softmax(X @ Ï€_k)
    logits = X @ pi_k  # (N, K)
    log_prior_z = torch.log_softmax(logits, dim=1)  # log p(Z)
    elbo += (tau * log_prior_z).sum()

    # --- 3. Entropy of Ï„: - Ï„ * log Ï„
    elbo -= (tau * torch.log(tau + 1e-10)).sum()

    # --- 4. Entropy of Î½: - Î½ * log Î½
    elbo -= (nu * torch.log(nu + 1e-10)).sum()

    # --- 5. Prior over Î½: Î½ * log Ï_ks
    log_rho = torch.log(rho_ks + 1e-10)  # (K, Q)
    log_rho_expanded = log_rho.unsqueeze(1).expand(K, p, Q)
    elbo += (nu * log_rho_expanded).sum()

    return elbo.item()
```

